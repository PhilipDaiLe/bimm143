---
title: "Class08"
author: "Philip Dai Le"
date: "10/24/2019"
output: github_document
---

-unsupervised learning: learning based on finding patterns or structure in unlabeled data

-supervised learning: making predictions based on label data; predict based on classification

-Reinforcement learning: learning based on past experience; like self-driving cars

LECTURE 8:10/24/19 Clustering
-K-means clustering: groups up numbers/info into groups of K

  -K assigns random initial cluster starts and measures the distance
between them then compare the relative distances to make a good group

  -K also looks at total variance of the clusters until every point is assigned
  
  -K will know the best answer by measuring all iterations with different variations
  
-Adding additional dimensions

  -Same like K, will pick random points and measure distances then group based on best deviation/variance
  
  -K-means: random component
  -X:numeric matrix or data frane
  -Look for elbow point of scree plot for best K; indicates best clustering
  
  
##K-means clustering
```{r}
# Generate some example data for clustering
tmp <- c(rnorm(30,-3), rnorm(30,3))
#30,-3 will be one cluster of 30 at -3 and vice verse for 30,3
x <- cbind(x=tmp, y=rev(tmp))
plot(x)

```
Use the kmeans() function setting k to 2 and nstart=20
Inspect/print the results
Q. How many points are in each cluster?
30 points

Q. What ‘component’ of your result object details
 - cluster size? 
 #"$" will denote specific factors of the function
 #k$size
 30 30
 
 - cluster assignment/membership?
 #k$cluster
  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1
[32] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

 - cluster center?
 #k$centers
           x         y
1 -3.003845  3.038557
2  3.038557 -3.003845

Plot x colored by the kmeans cluster assignment and
 add cluster centers as blue points
 
```{r}
k<-kmeans(x,centers= 2, nstart=20)
k$size
#size = number of points/value

k$cluster
#cluster means: cluster centers

k$centers

plot(x, col=k$cluster,)
#can use numbers to specific colors or color by cluster

points(k$centers, col="green", pch=15)
#pch makes the center solid

```
 
Disadvantage of K is having multiple iterations and deciphering which is best, so user imposing ruling/parameter = error

Hierarchical Clustering
-initially each point is treated as its own cluster
-Then looking for points closest together
-Then look more next closest
-Eventually start grouping pairs of points together until one big cluster

Significantly more flexibile, but more work
-need to input distance

##hierarchical clustering in R
hclust() function requires distance matrix as input. Use "dist()" function as input
```{r}
# First we need to calculate point (dis)similarity
# as the Euclidean distance between observations
dist_matrix <- dist(x)
# The hclust() function returns a hierarchical
# clustering model
hc <- hclust(d = dist_matrix)
# the print method is not so useful here
hc
 
hclust(d = dist_matrix)

plot(hc)
abline(h=6, col="magenta")
#adds in a colored line at height H or by number of clusters k=#

cutree(hc, k=4)
#look at the biggest jump/gap between branches. These are big changes in variance thus good places to cut
```

##Linking Clusters
Complete - links clusters based on largest distance between points
Single - links clusters based on smallest distance

```{r}
# Step 1. Generate some example data for clustering
x <- rbind(
 matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1
 matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
 matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
 rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
# Step 2. Plot the data without clustering
plot(x)
# Step 3. Generate colors for known clusters
# (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)


```

```{r}
#clustering code
h<-hclust(dist(x))

#draw tree code
plot(h)
abline(h=1.75, col="green")

#Cut tree into clusters
grps<-cutree(h, k=3)
grps


```

How many points in each cluster?
```{r}
table(grps)
```

Cross-tabulate i.e. compare our clustering results with the known answer
```{r}
table(grps, col)
```

## Dimensional Analysis
**PCA** - Principal component Analysis useful for observing a high dimensional data set and reduce it down to principal components or most key information

```{r}
mydata <- read.csv("https://tinyurl.com/expression-CSV",
 row.names=1)

head(mydata)

pca<-prcomp(t(mydata), scale = TRUE)
#"t" is to transpose the data because genes has to be columns not rows
dim(mydata)
#100 genes and 10 experiments
nrow(mydata)
attributes(pca)

pca$x
plot(pca$x[,1], pca$x[,2])
pca.var<-pca$sdev^2
pca.var.per <-round(pca.var/sum(pca.var)*100, 1) #the 1 dictates sigfig count
pca.var.per
barplot(pca.var.per, main="Scree Plot",
 xlab="Principal Component", ylab="Percent Variation")
```

```{r}
colvec <- colnames(mydata)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"
plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
 xlab=paste0("PC1 (", pca.var.per[1], "%)"),
 ylab=paste0("PC2 (", pca.var.per[2], "%)")) 
```

```{r}
plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
 xlab=paste0("PC1 (", pca.var.per[1], "%)"),
 ylab=paste0("PC2 (", pca.var.per[2], "%)"))
## Click to identify which sample is which
identify(pca$x[,1], pca$x[,2], labels=colnames(mydata))
## Press ESC to exit… 
```

```{r}
x <- read.csv("UK_foods.csv")
dim(x)
head(x)
rownames(x) <-x[,1]
x<-x[,-1] #dangerous way due to will constantly reiterate -1 every time you run it
head(x)
dim(x)
x<-read.csv("UK_foods.csv", row.names=1)
head(x)
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
pca<-prcomp(t(x))
summary(pca)
plot(pca$x[,1],pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1],pca$x[,2], colnames(x))
```
Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

17

Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

My preferred method would be to code in the row.name argument when initializing X




